{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Full System Stress Test\n",
    "\n",
    "This notebook exercises the full PINN Greeks stack â€” environment, data integrity, baselines, training, \n",
    "evaluation, and diagnostics. Run the cells top-to-bottom whenever you need a regression-style sanity check.\n",
    "\n",
    "**Sections**\n",
    "1. Environment setup and project paths\n",
    "2. Configuration and reproducibility controls\n",
    "3. Dataset availability and validation\n",
    "4. Baseline estimators vs analytic Greeks\n",
    "5. PINN training / checkpoint loading\n",
    "6. Quantitative evaluation on validation data\n",
    "7. PDE residual and surface diagnostics\n",
    "8. Summary + JSON export for logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dcaf0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/amv10802/Documents/Neural-PDE-Option-Greeks\n",
      "{'python': '3.11.7', 'numpy': '2.3.3', 'pandas': '2.3.3', 'matplotlib': '3.10.7', 'torch': '2.8.0', 'device_default': 'cpu'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 0) Environment & folders\n",
    "import os, sys, json, math, random, time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.dpi'] = 120\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    TORCH_OK = True\n",
    "except Exception as e:\n",
    "    TORCH_OK = False\n",
    "    print(\"PyTorch import failed:\", e)\n",
    "\n",
    "NOTEBOOK_CWD = Path.cwd().resolve()\n",
    "CANDIDATES = [NOTEBOOK_CWD, NOTEBOOK_CWD.parent, NOTEBOOK_CWD.parent.parent]\n",
    "BASE = None\n",
    "for candidate in CANDIDATES:\n",
    "    if (candidate / \"src\").is_dir():\n",
    "        BASE = candidate\n",
    "        break\n",
    "\n",
    "if BASE is None:\n",
    "    raise RuntimeError(f\"Could not locate project root from {NOTEBOOK_CWD}\")\n",
    "\n",
    "print(f\"Project root: {BASE}\")\n",
    "if str(BASE) not in sys.path:\n",
    "    sys.path.insert(0, str(BASE))\n",
    "\n",
    "for d in [\n",
    "    'src', 'src/utils', 'src/models', 'src/baselines',\n",
    "    'data', 'results', 'figures', 'figures/data_exploration',\n",
    "    'figures/training_curves', 'figures/residual_heatmaps', 'figures/final_results',\n",
    "    'figures/stress_test'\n",
    "]:\n",
    "    (BASE / d).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_DIR = BASE / 'data'\n",
    "FIGURES_DIR = BASE / 'figures'\n",
    "RESULTS_DIR = BASE / 'results'\n",
    "REPORTS_DIR = BASE / 'reports'\n",
    "REPORTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "SEED = 123\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "if TORCH_OK:\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "    DEFAULT_DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "else:\n",
    "    DEFAULT_DEVICE = 'cpu'\n",
    "\n",
    "print({\n",
    "    'python': sys.version.split()[0],\n",
    "    'numpy': np.__version__,\n",
    "    'pandas': pd.__version__,\n",
    "    'matplotlib': matplotlib.__version__,\n",
    "    'torch': torch.__version__ if TORCH_OK else None,\n",
    "    'device_default': DEFAULT_DEVICE,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cba1975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_epochs': 10,\n",
       " 'batch_size': 4096,\n",
       " 'learning_rate': 0.001,\n",
       " 'warmup_steps': 500,\n",
       " 'grad_clip': 1.0,\n",
       " 'baseline_samples': 1000,\n",
       " 'mc_paths': 20000,\n",
       " 'mc_eval_points': 25,\n",
       " 'num_val_samples': 5000,\n",
       " 'residual_grid': 40,\n",
       " 'load_existing_checkpoint': True,\n",
       " 'save_checkpoint': False,\n",
       " 'checkpoint_name': 'pinn_checkpoint.pt',\n",
       " 'device': 'cpu'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 1) Configuration (edit values here for quick experiments)\n",
    "CONFIG = {\n",
    "    'train_epochs': 10,\n",
    "    'batch_size': 4096,\n",
    "    'learning_rate': 1e-3,\n",
    "    'warmup_steps': 500,\n",
    "    'grad_clip': 1.0,\n",
    "    'baseline_samples': 1000,\n",
    "    'mc_paths': 20000,\n",
    "    'mc_eval_points': 25,\n",
    "    'num_val_samples': 5000,\n",
    "    'residual_grid': 40,\n",
    "    'load_existing_checkpoint': True,\n",
    "    'save_checkpoint': False,\n",
    "    'checkpoint_name': 'pinn_checkpoint.pt',\n",
    "    'device': DEFAULT_DEVICE,\n",
    "}\n",
    "\n",
    "CONFIG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e78bcfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing synthetic datasets.\n",
      "{'train_samples': 1000000, 'val_samples': 100000, 'train_path': '/Users/amv10802/Documents/Neural-PDE-Option-Greeks/data/synthetic_train.npy', 'val_path': '/Users/amv10802/Documents/Neural-PDE-Option-Greeks/data/synthetic_val.npy'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2) Dataset availability + loading (train/val)\n",
    "from src.data import generate_dataset\n",
    "\n",
    "train_path = DATA_DIR / 'synthetic_train.npy'\n",
    "val_path = DATA_DIR / 'synthetic_val.npy'\n",
    "\n",
    "if not train_path.exists() or not val_path.exists():\n",
    "    print(\"Synthetic dataset not found â€” generating via src.data.generate_dataset()\")\n",
    "    generate_dataset(output_dir=DATA_DIR)\n",
    "else:\n",
    "    print(\"Found existing synthetic datasets.\")\n",
    "\n",
    "train_data = np.load(train_path)\n",
    "val_data = np.load(val_path)\n",
    "\n",
    "columns = ['S', 't', 'sigma', 'V']\n",
    "train_df = pd.DataFrame(train_data, columns=columns)\n",
    "val_df = pd.DataFrame(val_data, columns=columns)\n",
    "\n",
    "print({\n",
    "    'train_samples': len(train_df),\n",
    "    'val_samples': len(val_df),\n",
    "    'train_path': str(train_path),\n",
    "    'val_path': str(val_path),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69093fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train summary:\n",
      "                    S               t           sigma             V\n",
      "count  1000000.000000  1000000.000000  1000000.000000  1.000000e+06\n",
      "mean       110.060208        1.003963        0.325066  3.382769e+01\n",
      "std         51.946403        0.574145        0.158812  3.422119e+01\n",
      "min         20.000093        0.010001        0.050001  0.000000e+00\n",
      "1%          21.804753        0.029708        0.055495  3.280156e-88\n",
      "5%          29.087468        0.108898        0.077477  6.087755e-14\n",
      "50%        110.100075        1.002917        0.325225  2.256246e+01\n",
      "95%        191.048427        1.900753        0.572594  9.711937e+01\n",
      "99%        198.206258        1.979981        0.594453  1.055692e+02\n",
      "max        199.999697        1.999999        0.599999  1.189780e+02\n",
      "Validation summary:\n",
      "                   S              t          sigma             V\n",
      "count  100000.000000  100000.000000  100000.000000  1.000000e+05\n",
      "mean      110.348122       1.005956       0.324366  3.396204e+01\n",
      "std        51.884131       0.574683       0.158816  3.420425e+01\n",
      "min        20.001565       0.010003       0.050009  0.000000e+00\n",
      "1%         21.909249       0.029612       0.055525  7.850025e-86\n",
      "5%         29.361669       0.109978       0.076979  7.600184e-14\n",
      "50%       110.721457       1.003972       0.324491  2.300670e+01\n",
      "95%       191.125306       1.899539       0.572041  9.717252e+01\n",
      "99%       198.216888       1.979710       0.594376  1.054815e+02\n",
      "max       199.997837       1.999985       0.599993  1.187141e+02\n",
      "NaN counts (train): {'S': np.int64(0), 't': np.int64(0), 'sigma': np.int64(0), 'V': np.int64(0)}\n",
      "NaN counts (val): {'S': np.int64(0), 't': np.int64(0), 'sigma': np.int64(0), 'V': np.int64(0)}\n",
      "S range: 20.00 â†’ 200.00\n",
      "t range: 0.0100 â†’ 2.0000\n",
      "sigma range: 0.0500 â†’ 0.6000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3) Dataset diagnostics\n",
    "summary_cols = ['S', 't', 'sigma', 'V']\n",
    "train_summary = train_df[summary_cols].describe(percentiles=[0.01, 0.05, 0.5, 0.95, 0.99])\n",
    "val_summary = val_df[summary_cols].describe(percentiles=[0.01, 0.05, 0.5, 0.95, 0.99])\n",
    "\n",
    "print(\"Train summary:\")\n",
    "print(train_summary)\n",
    "print(\"Validation summary:\")\n",
    "print(val_summary)\n",
    "\n",
    "nan_train = np.isnan(train_data).sum(axis=0)\n",
    "nan_val = np.isnan(val_data).sum(axis=0)\n",
    "print(\"NaN counts (train):\", dict(zip(summary_cols, nan_train)))\n",
    "print(\"NaN counts (val):\", dict(zip(summary_cols, nan_val)))\n",
    "\n",
    "# Stress test boundary conditions\n",
    "S_min, S_max = train_df['S'].min(), train_df['S'].max()\n",
    "print(f\"S range: {S_min:.2f} â†’ {S_max:.2f}\")\n",
    "print(f\"t range: {train_df['t'].min():.4f} â†’ {train_df['t'].max():.4f}\")\n",
    "print(f\"sigma range: {train_df['sigma'].min():.4f} â†’ {train_df['sigma'].max():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53298489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finite-difference delta MAE: 0.000000\n",
      "Finite-difference gamma MAE: 0.000000\n",
      "Monte Carlo delta summary:\n",
      "                S   mc_delta  analytic_delta    abs_err\n",
      "count   25.000000  25.000000       25.000000  25.000000\n",
      "mean   100.000000   0.539672        0.539240   0.001780\n",
      "std     36.799004   0.412345        0.412186   0.001857\n",
      "min     40.000000   0.000000        0.000012   0.000012\n",
      "25%     70.000000   0.077521        0.075875   0.000417\n",
      "50%    100.000000   0.642709        0.636831   0.001321\n",
      "75%    130.000000   0.950405        0.951726   0.002056\n",
      "max    160.000000   0.996094        0.996533   0.006395\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4) Baseline estimators vs analytic Greeks\n",
    "from src.utils.black_scholes import bs_price, bs_greeks\n",
    "from src.baselines.finite_difference import finite_diff_greeks\n",
    "from src.baselines.monte_carlo import mc_pathwise_delta\n",
    "\n",
    "K = 100.0\n",
    "T = 2.0\n",
    "r = 0.05\n",
    "snapshot_t = 1.0\n",
    "snapshot_sigma = 0.2\n",
    "\n",
    "S_eval = np.linspace(20, 200, CONFIG['baseline_samples'])\n",
    "fd_delta, fd_gamma = finite_diff_greeks(S_eval, K=K, T=T, t=snapshot_t, sigma=snapshot_sigma, r=r)\n",
    "analytic = bs_greeks(S_eval, K, T, snapshot_t, snapshot_sigma, r)\n",
    "analytic_delta = analytic['delta']\n",
    "analytic_gamma = analytic['gamma']\n",
    "\n",
    "fd_delta_mae = np.mean(np.abs(fd_delta - analytic_delta))\n",
    "fd_gamma_mae = np.mean(np.abs(fd_gamma - analytic_gamma))\n",
    "\n",
    "mc_points = np.linspace(40, 160, CONFIG['mc_eval_points'])\n",
    "mc_results = []\n",
    "for idx, S0 in enumerate(mc_points):\n",
    "    est = mc_pathwise_delta(S0, K=K, T=snapshot_t, r=r, sigma=snapshot_sigma, N=CONFIG['mc_paths'], seed=idx)\n",
    "    target = bs_greeks(S0, K, T, snapshot_t, snapshot_sigma, r)['delta']\n",
    "    mc_results.append({'S': S0, 'mc_delta': est, 'analytic_delta': target, 'abs_err': abs(est - target)})\n",
    "mc_df = pd.DataFrame(mc_results)\n",
    "\n",
    "print(f\"Finite-difference delta MAE: {fd_delta_mae:.6f}\")\n",
    "print(f\"Finite-difference gamma MAE: {fd_gamma_mae:.6f}\")\n",
    "print(\"Monte Carlo delta summary:\")\n",
    "print(mc_df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c896f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | loss=1357.592095 | price=1355.037248 | pde=2.554317 | reg=0.000534\n",
      "Epoch 002 | loss=1303.461871 | price=1293.289988 | pde=10.171536 | reg=0.000351\n",
      "Epoch 003 | loss=1280.109765 | price=1279.041850 | pde=1.067637 | reg=0.000284\n",
      "Epoch 004 | loss=1238.052212 | price=1237.904972 | pde=0.147035 | reg=0.000204\n",
      "Epoch 005 | loss=1194.792098 | price=1194.702432 | pde=0.089504 | reg=0.000150\n",
      "Epoch 006 | loss=1185.102789 | price=1184.995081 | pde=0.107610 | reg=0.000075\n",
      "Epoch 007 | loss=1183.288234 | price=1183.165130 | pde=0.123076 | reg=0.000052\n",
      "Epoch 008 | loss=1198.115457 | price=1198.002117 | pde=0.113275 | reg=0.000063\n",
      "Epoch 009 | loss=1186.720478 | price=1186.577019 | pde=0.143441 | reg=0.000045\n",
      "Epoch 010 | loss=1186.528250 | price=1186.397155 | pde=0.131061 | reg=0.000043\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>price</th>\n",
       "      <th>pde</th>\n",
       "      <th>reg</th>\n",
       "      <th>lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1357.592095</td>\n",
       "      <td>1355.037248</td>\n",
       "      <td>2.554317</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.00049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1303.461871</td>\n",
       "      <td>1293.289988</td>\n",
       "      <td>10.171536</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.00098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1280.109765</td>\n",
       "      <td>1279.041850</td>\n",
       "      <td>1.067637</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>0.00100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1238.052212</td>\n",
       "      <td>1237.904972</td>\n",
       "      <td>0.147035</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.00100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1194.792098</td>\n",
       "      <td>1194.702432</td>\n",
       "      <td>0.089504</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.00100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1185.102789</td>\n",
       "      <td>1184.995081</td>\n",
       "      <td>0.107610</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.00100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1183.288234</td>\n",
       "      <td>1183.165130</td>\n",
       "      <td>0.123076</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.00100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1198.115457</td>\n",
       "      <td>1198.002117</td>\n",
       "      <td>0.113275</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.00100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1186.720478</td>\n",
       "      <td>1186.577019</td>\n",
       "      <td>0.143441</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.00100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1186.528250</td>\n",
       "      <td>1186.397155</td>\n",
       "      <td>0.131061</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.00100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch         loss        price        pde       reg       lr\n",
       "0      1  1357.592095  1355.037248   2.554317  0.000534  0.00049\n",
       "1      2  1303.461871  1293.289988  10.171536  0.000351  0.00098\n",
       "2      3  1280.109765  1279.041850   1.067637  0.000284  0.00100\n",
       "3      4  1238.052212  1237.904972   0.147035  0.000204  0.00100\n",
       "4      5  1194.792098  1194.702432   0.089504  0.000150  0.00100\n",
       "5      6  1185.102789  1184.995081   0.107610  0.000075  0.00100\n",
       "6      7  1183.288234  1183.165130   0.123076  0.000052  0.00100\n",
       "7      8  1198.115457  1198.002117   0.113275  0.000063  0.00100\n",
       "8      9  1186.720478  1186.577019   0.143441  0.000045  0.00100\n",
       "9     10  1186.528250  1186.397155   0.131061  0.000043  0.00100"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 5) Train or load PINN model\n",
    "if not TORCH_OK:\n",
    "    model = None\n",
    "    training_log = []\n",
    "    print('PyTorch not available â€” skipping PINN training.')\n",
    "else:\n",
    "    from src.models import PINNModel\n",
    "    from src.losses import pinn_loss\n",
    "    from src.train import load_data\n",
    "\n",
    "    device = torch.device(CONFIG['device'])\n",
    "    checkpoint_path = RESULTS_DIR / CONFIG['checkpoint_name']\n",
    "    model = PINNModel().to(device)\n",
    "    training_log = []\n",
    "\n",
    "    if CONFIG['load_existing_checkpoint'] and checkpoint_path.exists():\n",
    "        model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "        print(f\"Loaded checkpoint: {checkpoint_path}\")\n",
    "    else:\n",
    "        loader = load_data(path=train_path, batch_size=CONFIG['batch_size'])\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])\n",
    "        base_lr = CONFIG['learning_rate']\n",
    "        warmup = max(0, CONFIG['warmup_steps'])\n",
    "        global_step = 0\n",
    "\n",
    "        for epoch in range(CONFIG['train_epochs']):\n",
    "            epoch_loss = 0.0\n",
    "            epoch_price = 0.0\n",
    "            epoch_pde = 0.0\n",
    "            epoch_reg = 0.0\n",
    "            steps = 0\n",
    "            model.train()\n",
    "            for batch in loader:\n",
    "                S_b, t_b, sigma_b, V_b = [x.to(device) for x in batch]\n",
    "                opt.zero_grad()\n",
    "                loss, (L_price, L_PDE, L_reg) = pinn_loss(model, S_b, t_b, sigma_b, V_b)\n",
    "                if warmup > 0:\n",
    "                    lr_scale = min(1.0, (global_step + 1) / warmup)\n",
    "                    for g in opt.param_groups:\n",
    "                        g['lr'] = base_lr * lr_scale\n",
    "                loss.backward()\n",
    "                if CONFIG['grad_clip'] and CONFIG['grad_clip'] > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['grad_clip'])\n",
    "                opt.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_price += L_price.item()\n",
    "                epoch_pde += L_PDE.item()\n",
    "                epoch_reg += L_reg.item()\n",
    "                steps += 1\n",
    "                global_step += 1\n",
    "\n",
    "            log_entry = {\n",
    "                'epoch': epoch + 1,\n",
    "                'loss': epoch_loss / steps,\n",
    "                'price': epoch_price / steps,\n",
    "                'pde': epoch_pde / steps,\n",
    "                'reg': epoch_reg / steps,\n",
    "                'lr': opt.param_groups[0]['lr'],\n",
    "            }\n",
    "            training_log.append(log_entry)\n",
    "            print(f\"Epoch {log_entry['epoch']:03d} | loss={log_entry['loss']:.6f} | \"\n",
    "                  f\"price={log_entry['price']:.6f} | pde={log_entry['pde']:.6f} | reg={log_entry['reg']:.6f}\")\n",
    "\n",
    "        if CONFIG['save_checkpoint']:\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "\n",
    "    if training_log:\n",
    "        training_df = pd.DataFrame(training_log)\n",
    "        display(training_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44383676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"N_eval\": 5000,\n",
      "  \"price_mae\": 31.619967691443946,\n",
      "  \"price_rmse\": 36.85272648283381,\n",
      "  \"delta_mae\": 0.6316764053012146,\n",
      "  \"gamma_mae\": 0.005521820800736746\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 6) Quantitative evaluation on validation data\n",
    "if not TORCH_OK or model is None:\n",
    "    evaluation_summary = None\n",
    "    print('Skipping evaluation step (model unavailable).')\n",
    "else:\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    N_eval = min(CONFIG['num_val_samples'], len(val_df))\n",
    "    idx = np.random.choice(len(val_df), size=N_eval, replace=False)\n",
    "    batch = val_data[idx]\n",
    "    S_np = batch[:, 0]\n",
    "    t_np = batch[:, 1]\n",
    "    sigma_np = batch[:, 2]\n",
    "    target_price = batch[:, 3]\n",
    "\n",
    "    S = torch.tensor(S_np, dtype=torch.float32, device=device, requires_grad=True)\n",
    "    t = torch.tensor(t_np, dtype=torch.float32, device=device, requires_grad=True)\n",
    "    sigma = torch.tensor(sigma_np, dtype=torch.float32, device=device, requires_grad=True)\n",
    "\n",
    "    inputs = torch.stack([S, t, sigma], dim=1)\n",
    "    inputs.requires_grad_(True)\n",
    "    preds = model(inputs).squeeze()\n",
    "\n",
    "    ones = torch.ones_like(preds)\n",
    "    grad = torch.autograd.grad(preds, inputs, grad_outputs=ones, create_graph=True)[0]\n",
    "    delta = grad[:, 0]\n",
    "    # Gamma: derivative of delta with respect to S\n",
    "    gamma = torch.autograd.grad(delta, inputs, grad_outputs=torch.ones_like(delta), create_graph=True)[0][:, 0]\n",
    "\n",
    "    preds_np = preds.detach().cpu().numpy()\n",
    "    delta_np = delta.detach().cpu().numpy()\n",
    "    gamma_np = gamma.detach().cpu().numpy()\n",
    "\n",
    "    analytic = bs_greeks(S_np, K, T, t_np, sigma_np, r)\n",
    "    analytic_delta = analytic['delta']\n",
    "    analytic_gamma = analytic['gamma']\n",
    "\n",
    "    price_mae = np.mean(np.abs(preds_np - target_price))\n",
    "    price_rmse = math.sqrt(np.mean((preds_np - target_price)**2))\n",
    "    delta_mae = np.mean(np.abs(delta_np - analytic_delta))\n",
    "    gamma_mae = np.mean(np.abs(gamma_np - analytic_gamma))\n",
    "\n",
    "    evaluation_summary = {\n",
    "        'N_eval': int(N_eval),\n",
    "        'price_mae': float(price_mae),\n",
    "        'price_rmse': float(price_rmse),\n",
    "        'delta_mae': float(delta_mae),\n",
    "        'gamma_mae': float(gamma_mae),\n",
    "    }\n",
    "\n",
    "    print(json.dumps(evaluation_summary, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a859a88",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 2560000 into shape (40,40)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     13\u001b[39m t_mesh = torch.full_like(S_mesh, snapshot_t)\n\u001b[32m     15\u001b[39m resid = compute_pde_residual(\n\u001b[32m     16\u001b[39m     model,\n\u001b[32m     17\u001b[39m     S_mesh.flatten(),\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m     r=r,\n\u001b[32m     21\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m resid_np = \u001b[43mresid\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m residual_stats = {\n\u001b[32m     25\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(resid_np.mean()),\n\u001b[32m     26\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mstd\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(resid_np.std()),\n\u001b[32m     27\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmax_abs\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(np.max(np.abs(resid_np))),\n\u001b[32m     28\u001b[39m }\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(json.dumps(residual_stats, indent=\u001b[32m2\u001b[39m))\n",
      "\u001b[31mValueError\u001b[39m: cannot reshape array of size 2560000 into shape (40,40)"
     ]
    }
   ],
   "source": [
    "\n",
    "# 7) PDE residual + surface diagnostics\n",
    "if not TORCH_OK or model is None:\n",
    "    residual_stats = None\n",
    "    print('Skipping PDE residual diagnostics (model unavailable).')\n",
    "else:\n",
    "    from src.losses import compute_pde_residual\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    G = CONFIG['residual_grid']\n",
    "    S_grid = torch.linspace(20, 200, G, device=device)\n",
    "    sigma_grid = torch.linspace(0.05, 0.6, G, device=device)\n",
    "    S_mesh, sigma_mesh = torch.meshgrid(S_grid, sigma_grid, indexing='ij')\n",
    "    t_mesh = torch.full_like(S_mesh, snapshot_t)\n",
    "\n",
    "    resid = compute_pde_residual(\n",
    "        model,\n",
    "        S_mesh.flatten(),\n",
    "        t_mesh.flatten(),\n",
    "        sigma_mesh.flatten(),\n",
    "        r=r,\n",
    "    )\n",
    "    resid_np = resid.detach().cpu().numpy().reshape(G, G)\n",
    "\n",
    "    residual_stats = {\n",
    "        'mean': float(resid_np.mean()),\n",
    "        'std': float(resid_np.std()),\n",
    "        'max_abs': float(np.max(np.abs(resid_np))),\n",
    "    }\n",
    "    print(json.dumps(residual_stats, indent=2))\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.contourf(S_grid.cpu().numpy(), sigma_grid.cpu().numpy(), resid_np.T, levels=40, cmap='coolwarm')\n",
    "    plt.colorbar(label='PDE residual')\n",
    "    plt.xlabel('S')\n",
    "    plt.ylabel('sigma')\n",
    "    plt.title('PDE residual heatmap (stress grid)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'stress_test' / 'pde_residual_heatmap.png', dpi=200)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        coords = torch.stack([\n",
    "            S_mesh.flatten(),\n",
    "            t_mesh.flatten(),\n",
    "            sigma_mesh.flatten(),\n",
    "        ], dim=1)\n",
    "        surface = model(coords).detach().cpu().numpy().reshape(G, G)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.contourf(S_grid.cpu().numpy(), sigma_grid.cpu().numpy(), surface.T, levels=40, cmap='viridis')\n",
    "    plt.colorbar(label='Price')\n",
    "    plt.xlabel('S')\n",
    "    plt.ylabel('sigma')\n",
    "    plt.title('PINN price surface (stress grid)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'stress_test' / 'surface_grid.png', dpi=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8) Consolidated summary + optional JSON export\n",
    "summary_payload = {\n",
    "    'timestamp': datetime.utcnow().isoformat() + 'Z',\n",
    "    'config': CONFIG,\n",
    "    'evaluation': evaluation_summary,\n",
    "    'residual': residual_stats,\n",
    "    'fd_delta_mae': float(fd_delta_mae),\n",
    "    'fd_gamma_mae': float(fd_gamma_mae),\n",
    "    'mc_delta_mean_abs_error': float(mc_df['abs_err'].mean()) if 'mc_df' in globals() else None,\n",
    "}\n",
    "\n",
    "print(json.dumps(summary_payload, indent=2))\n",
    "\n",
    "summary_path = RESULTS_DIR / 'stress_test_summary.json'\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary_payload, f, indent=2)\n",
    "print(f\"Wrote summary to {summary_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
